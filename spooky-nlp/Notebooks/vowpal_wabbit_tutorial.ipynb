{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "403f8973-1fb5-422c-bd82-fb5d7041d8b2",
    "_uuid": "091313c11c12aaeef8bc25b39a205f79bb2588a6",
    "collapsed": true
   },
   "source": [
    "# <center> Vowpal Wabbit tutorial: blazingly fast learning\n",
    "In this tutorial, we'll cover (both theoratically and in practice) two reasons of Vowpal Wabbit's exceptional training speed, namely, online learning and hashing trick. We'll try it out on the competition's data as well as with news, letters, movie reviews datasets and gigabytes of StackOverflow questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a783ea16-a0be-4a81-9d91-72c9ada412d0",
    "_uuid": "aba8923437f1bb67ff9615f218dbe3cb19145f4e"
   },
   "source": [
    "# Outline\n",
    "1. Stochastic gradient descent and online learning\n",
    "    - 1.1. SGD\n",
    "    - 1.2. Online approach to learning\n",
    "2. Categorical data processing: Label Encoding, One-Hot Encoding, Hashing trick\n",
    "    - 2.1. Label Encoding\n",
    "    - 2.2. One-Hot Encoding\n",
    "    - 2.3. Hashing trick\n",
    "3. Vowpal Wabbit\n",
    "    - 3.1. News. Binary classification\n",
    "    - 3.2. News. Multiclass classification\n",
    "    - 3.3. IMDB reviews\n",
    "    - 3.4. Classifying gigabytes of StackOverflow questions\n",
    "4. VW and Spooky Author Identification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "64efb6ef-3591-4036-8461-b7b467f404b7",
    "_uuid": "bd1f11458028d021571d4a77e3de846bf0dee992"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.datasets import fetch_20newsgroups, load_files\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ddb2276a-dd17-4229-8a0b-1860056dc08a",
    "_uuid": "8c3581ddac23e8621f7688aec41bbce23e91bc6c"
   },
   "source": [
    "# 1. Stochastic gradient descent and online learning\n",
    "##  1.1. Stochastic gradient descent\n",
    "\n",
    "Despite the fact that gradient descent is one of the first things learned in machine learning and optimization courses, it is hard to overrate one of it's modifications, namely, Stochastic Gradient Descent (SGD).\n",
    "\n",
    "Lets recap that the very idea of gradient descent is to minimize some function by making small steps in the direction of fastest function decreasing. The method was named due to the following fact from calculus: vector $\\nabla f = (\\frac{\\partial f}{\\partial x_1}, \\ldots \\frac{\\partial f}{\\partial x_n})^T$ of partial derivatives of the function $f(x) = f(x_1, \\ldots x_n)$ points to the direction of the fastest function growth. It means that by moving in the opposite direction (antigradient) it is possible to decrease the function value with the fastest rate.\n",
    "\n",
    "<img src='https://habrastorage.org/files/4f2/75d/a46/4f275da467a44fc4a8d1a11007776ed2.jpg' width=70%>\n",
    "\n",
    "Here is a snowboarder (me) in Sheregesh, Russian most popular winter resort. I highly recommended it if you like skiing or snowboarding. We place this picture not only for a good view but also for picturing the idea of gradient descent. If you have an aim to ride as fast as possible, you need to choose the way with steepest descent (as long as you stay alive). Calculating antigradient can be seen as evaluating the slope in each particular point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eda14ceb-ab19-4c75-a1ad-bbbbf3acb83d",
    "_uuid": "7c41b31a7549e4dbcd0c31824d425d18ec6b9fbb"
   },
   "source": [
    "**Example**\n",
    "\n",
    "The paired regression problem can be solved with gradient descent. Let us predict one variable with another, say, height with weight and assume that these variables are lineary dependent. Here we are using the SOCR dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "84022022-7ce2-4fac-90ae-3cefc5a12961",
    "_uuid": "0873cf8a28158f2ebdd5b3b42d3026efcd8e6b58"
   },
   "outputs": [],
   "source": [
    "PATH_TO_ALL_DATA = '../Data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d2361062-61d9-4dd6-bac2-cbf2bd0ccebb",
    "_uuid": "1a565b5d7413123af2b5997cd9f74266187effca"
   },
   "source": [
    "# 3. Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6a2e0f9-785e-4cf6-9013-b114099daddd",
    "_uuid": "01e502198f14273eadcab0f10e090ddbaba07b87"
   },
   "source": [
    "[Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit) (VW) is one of the most wide-spread machine learning libraries in industry. It is prominent for high training speed and support of many training modes. Especially, we are interested in online learning with big and high-dimentional data. This is one of major merits of the library. Also, with hashing trick implemented, Vowpal Wabbit is a perfect choice for working with text data.\n",
    "\n",
    "Shell is the main interface for VW. Well... I haven't found the way of installing VW in a Kaggle Kernel (hmm.. Kaggle, what about Docker?) so I've commented out the code in some cells in order not to spoil the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "24fd028f-416e-43ff-9530-5c8f47bde176",
    "_uuid": "4cc17ec6551c7e7dcf224a3b03794720d90cb966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = \r\n",
      "num sources = 1\r\n",
      "\r\n",
      "\r\n",
      "VW options:\r\n",
      "  --random_seed arg                     seed random number generator\r\n",
      "  --ring_size arg                       size of example ring\r\n",
      "\r\n",
      "Update options:\r\n",
      "  -l [ --learning_rate ] arg            Set learning rate\r\n",
      "  --power_t arg                         t power value\r\n",
      "  --decay_learning_rate arg             Set Decay factor for learning_rate \r\n",
      "                                        between passes\r\n",
      "  --initial_t arg                       initial t value\r\n",
      "  --feature_mask arg                    Use existing regressor to determine \r\n",
      "                                        which parameters may be updated.  If no\r\n",
      "                                        initial_regressor given, also used for \r\n",
      "                                        initial weights.\r\n",
      "\r\n",
      "Weight options:\r\n",
      "  -i [ --initial_regressor ] arg        Initial regressor(s)\r\n",
      "  --initial_weight arg                  Set all weights to an initial value of \r\n",
      "                                        arg.\r\n",
      "  --random_weights arg                  make initial weights random\r\n",
      "  --normal_weights arg                  make initial weights normal\r\n",
      "  --truncated_normal_weights arg        make initial weights truncated normal\r\n",
      "  --sparse_weights                      Use a sparse datastructure for weights\r\n",
      "  --input_feature_regularizer arg       Per feature regularization input file\r\n",
      "\r\n",
      "Parallelization options:\r\n",
      "  --span_server arg                     Location of server for setting up \r\n",
      "                                        spanning tree\r\n",
      "  --threads                             Enable multi-threading\r\n",
      "  --unique_id arg (=0)                  unique id used for cluster parallel \r\n",
      "                                        jobs\r\n",
      "  --total arg (=1)                      total number of nodes used in cluster \r\n",
      "                                        parallel job\r\n",
      "  --node arg (=0)                       node number in cluster parallel job\r\n",
      "\r\n",
      "Diagnostic options:\r\n",
      "  --version                             Version information\r\n",
      "  -a [ --audit ]                        print weights of features\r\n",
      "  -P [ --progress ] arg                 Progress update frequency. int: \r\n",
      "                                        additive, float: multiplicative\r\n",
      "  --quiet                               Don't output disgnostics and progress \r\n",
      "                                        updates\r\n",
      "  -h [ --help ]                         Look here: http://hunch.net/~vw/ and \r\n",
      "                                        click on Tutorial.\r\n",
      "\r\n",
      "Feature options:\r\n",
      "  --hash arg                            how to hash the features. Available \r\n",
      "                                        options: strings, all\r\n",
      "  --ignore arg                          ignore namespaces beginning with \r\n",
      "                                        character <arg>\r\n",
      "  --ignore_linear arg                   ignore namespaces beginning with \r\n",
      "                                        character <arg> for linear terms only\r\n",
      "  --keep arg                            keep namespaces beginning with \r\n",
      "                                        character <arg>\r\n",
      "  --redefine arg                        redefine namespaces beginning with \r\n",
      "                                        characters of string S as namespace N. \r\n",
      "                                        <arg> shall be in form 'N:=S' where := \r\n",
      "                                        is operator. Empty N or S are treated \r\n",
      "                                        as default namespace. Use ':' as a \r\n",
      "                                        wildcard in S.\r\n",
      "  -b [ --bit_precision ] arg            number of bits in the feature table\r\n",
      "  --noconstant                          Don't add a constant feature\r\n",
      "  -C [ --constant ] arg                 Set initial value of constant\r\n",
      "  --ngram arg                           Generate N grams. To generate N grams \r\n",
      "                                        for a single namespace 'foo', arg \r\n",
      "                                        should be fN.\r\n",
      "  --skips arg                           Generate skips in N grams. This in \r\n",
      "                                        conjunction with the ngram tag can be \r\n",
      "                                        used to generate generalized \r\n",
      "                                        n-skip-k-gram. To generate n-skips for \r\n",
      "                                        a single namespace 'foo', arg should be\r\n",
      "                                        fN.\r\n",
      "  --feature_limit arg                   limit to N features. To apply to a \r\n",
      "                                        single namespace 'foo', arg should be \r\n",
      "                                        fN\r\n",
      "  --affix arg                           generate prefixes/suffixes of features;\r\n",
      "                                        argument '+2a,-3b,+1' means generate \r\n",
      "                                        2-char prefixes for namespace a, 3-char\r\n",
      "                                        suffixes for b and 1 char prefixes for \r\n",
      "                                        default namespace\r\n",
      "  --spelling arg                        compute spelling features for a give \r\n",
      "                                        namespace (use '_' for default \r\n",
      "                                        namespace)\r\n",
      "  --dictionary arg                      read a dictionary for additional \r\n",
      "                                        features (arg either 'x:file' or just \r\n",
      "                                        'file')\r\n",
      "  --dictionary_path arg                 look in this directory for \r\n",
      "                                        dictionaries; defaults to current \r\n",
      "                                        directory or env{PATH}\r\n",
      "  --interactions arg                    Create feature interactions of any \r\n",
      "                                        level between namespaces.\r\n",
      "  --permutations                        Use permutations instead of \r\n",
      "                                        combinations for feature interactions \r\n",
      "                                        of same namespace.\r\n",
      "  --leave_duplicate_interactions        Don't remove interactions with \r\n",
      "                                        duplicate combinations of namespaces. \r\n",
      "                                        For ex. this is a duplicate: '-q ab -q \r\n",
      "                                        ba' and a lot more in '-q ::'.\r\n",
      "  -q [ --quadratic ] arg                Create and use quadratic features\r\n",
      "  --q: arg                              : corresponds to a wildcard for all \r\n",
      "                                        printable characters\r\n",
      "  --cubic arg                           Create and use cubic features\r\n",
      "\r\n",
      "Example options:\r\n",
      "  -t [ --testonly ]                     Ignore label information and just test\r\n",
      "  --holdout_off                         no holdout data in multiple passes\r\n",
      "  --holdout_period arg                  holdout period for test only, default \r\n",
      "                                        10\r\n",
      "  --holdout_after arg                   holdout after n training examples, \r\n",
      "                                        default off (disables holdout_period)\r\n",
      "  --early_terminate arg                 Specify the number of passes tolerated \r\n",
      "                                        when holdout loss doesn't decrease \r\n",
      "                                        before early termination, default is 3\r\n",
      "  --passes arg                          Number of Training Passes\r\n",
      "  --initial_pass_length arg             initial number of examples per pass\r\n",
      "  --examples arg                        number of examples to parse\r\n",
      "  --min_prediction arg                  Smallest prediction to output\r\n",
      "  --max_prediction arg                  Largest prediction to output\r\n",
      "  --sort_features                       turn this on to disregard order in \r\n",
      "                                        which features have been defined. This \r\n",
      "                                        will lead to smaller cache sizes\r\n",
      "  --loss_function arg (=squared)        Specify the loss function to be used, \r\n",
      "                                        uses squared by default. Currently \r\n",
      "                                        available ones are squared, classic, \r\n",
      "                                        hinge, logistic, quantile and poisson.\r\n",
      "  --quantile_tau arg (=0.5)             Parameter \\tau associated with Quantile\r\n",
      "                                        loss. Defaults to 0.5\r\n",
      "  --l1 arg                              l_1 lambda\r\n",
      "  --l2 arg                              l_2 lambda\r\n",
      "  --no_bias_regularization arg          no bias in regularization\r\n",
      "  --named_labels arg                    use names for labels (multiclass, etc.)\r\n",
      "                                        rather than integers, argument \r\n",
      "                                        specified all possible labels, \r\n",
      "                                        comma-sep, eg \"--named_labels \r\n",
      "                                        Noun,Verb,Adj,Punc\"\r\n",
      "\r\n",
      "Output model:\r\n",
      "  -f [ --final_regressor ] arg          Final regressor\r\n",
      "  --readable_model arg                  Output human-readable final regressor \r\n",
      "                                        with numeric features\r\n",
      "  --invert_hash arg                     Output human-readable final regressor \r\n",
      "                                        with feature names.  Computationally \r\n",
      "                                        expensive.\r\n",
      "  --save_resume                         save extra state so learning can be \r\n",
      "                                        resumed later with new data\r\n",
      "  --preserve_performance_counters       reset performance counters when \r\n",
      "                                        warmstarting\r\n",
      "  --save_per_pass                       Save the model after every pass over \r\n",
      "                                        data\r\n",
      "  --output_feature_regularizer_binary arg\r\n",
      "                                        Per feature regularization output file\r\n",
      "  --output_feature_regularizer_text arg Per feature regularization output file,\r\n",
      "                                        in text\r\n",
      "  --id arg                              User supplied ID embedded into the \r\n",
      "                                        final regressor\r\n",
      "\r\n",
      "Output options:\r\n",
      "  -p [ --predictions ] arg              File to output predictions to\r\n",
      "  -r [ --raw_predictions ] arg          File to output unnormalized predictions\r\n",
      "                                        to\r\n",
      "\r\n",
      "Reduction options, use [option] --help for more info:\r\n",
      "\r\n",
      "  --audit_regressor arg                 stores feature names and their \r\n",
      "                                        regressor values. Same dataset must be \r\n",
      "                                        used for both regressor training and \r\n",
      "                                        this mode.\r\n",
      "\r\n",
      "  --search arg                          Use learning to search, \r\n",
      "                                        argument=maximum action id or 0 for LDF\r\n",
      "\r\n",
      "  --replay_c arg                        use experience replay at a specified \r\n",
      "                                        level [b=classification/regression, \r\n",
      "                                        m=multiclass, c=cost sensitive] with \r\n",
      "                                        specified buffer size\r\n",
      "\r\n",
      "  --explore_eval                        Evaluate explore_eval adf policies\r\n",
      "\r\n",
      "  --cbify arg                           Convert multiclass on <k> classes into \r\n",
      "                                        a contextual bandit problem\r\n",
      "\r\n",
      "  --cb_explore_adf                      Online explore-exploit for a contextual\r\n",
      "                                        bandit problem with multiline action \r\n",
      "                                        dependent features\r\n",
      "\r\n",
      "  --cb_explore arg                      Online explore-exploit for a <k> action\r\n",
      "                                        contextual bandit problem\r\n",
      "\r\n",
      "  --multiworld_test arg                 Evaluate features as a policies\r\n",
      "\r\n",
      "  --cb_adf                              Do Contextual Bandit learning with \r\n",
      "                                        multiline action dependent features.\r\n",
      "\r\n",
      "  --cb arg                              Use contextual bandit learning with <k>\r\n",
      "                                        costs\r\n",
      "\r\n",
      "  --csoaa_ldf arg                       Use one-against-all multiclass learning\r\n",
      "                                        with label dependent features.  Specify\r\n",
      "                                        singleline or multiline.\r\n",
      "\r\n",
      "  --wap_ldf arg                         Use weighted all-pairs multiclass \r\n",
      "                                        learning with label dependent features.\r\n",
      "                                          Specify singleline or multiline.\r\n",
      "\r\n",
      "  --interact arg                        Put weights on feature products from \r\n",
      "                                        namespaces <n1> and <n2>\r\n",
      "\r\n",
      "  --csoaa arg                           One-against-all multiclass with <k> \r\n",
      "                                        costs\r\n",
      "\r\n",
      "  --cs_active arg                       Cost-sensitive active learning with <k>\r\n",
      "                                        costs\r\n",
      "\r\n",
      "  --multilabel_oaa arg                  One-against-all multilabel with <k> \r\n",
      "                                        labels\r\n",
      "\r\n",
      "  --classweight arg                     importance weight multiplier for class\r\n",
      "\r\n",
      "  --recall_tree arg                     Use online tree for multiclass\r\n",
      "\r\n",
      "  --log_multi arg                       Use online tree for multiclass\r\n",
      "\r\n",
      "  --ect arg                             Error correcting tournament with <k> \r\n",
      "                                        labels\r\n",
      "\r\n",
      "  --boosting arg                        Online boosting with <N> weak learners\r\n",
      "\r\n",
      "  --oaa arg                             One-against-all multiclass with <k> \r\n",
      "                                        labels\r\n",
      "\r\n",
      "  --top arg                             top k recommendation\r\n",
      "\r\n",
      "  --replay_m arg                        use experience replay at a specified \r\n",
      "                                        level [b=classification/regression, \r\n",
      "                                        m=multiclass, c=cost sensitive] with \r\n",
      "                                        specified buffer size\r\n",
      "\r\n",
      "  --binary                              report loss as binary classification on\r\n",
      "                                        -1,1\r\n",
      "\r\n",
      "  --bootstrap arg                       k-way bootstrap by online importance \r\n",
      "                                        resampling\r\n",
      "\r\n",
      "  --link arg (=identity)                Specify the link function: identity, \r\n",
      "                                        logistic, glf1 or poisson\r\n",
      "\r\n",
      "  --stage_poly                          use stagewise polynomial feature \r\n",
      "                                        learning\r\n",
      "\r\n",
      "  --lrqfa arg                           use low rank quadratic features with \r\n",
      "                                        field aware weights\r\n",
      "\r\n",
      "  --lrq arg                             use low rank quadratic features\r\n",
      "\r\n",
      "  --autolink arg                        create link function with polynomial d\r\n",
      "\r\n",
      "  --marginal arg                        substitute marginal label estimates for\r\n",
      "                                        ids\r\n",
      "\r\n",
      "  --new_mf arg                          rank for reduction-based matrix \r\n",
      "                                        factorization\r\n",
      "\r\n",
      "  --nn arg                              Sigmoidal feedforward network with <k> \r\n",
      "                                        hidden units\r\n",
      "\r\n",
      "confidence options:\r\n",
      "  --confidence_after_training           Confidence after training\r\n",
      "\r\n",
      "  --confidence                          Get confidence for binary predictions\r\n",
      "\r\n",
      "  --active_cover                        enable active learning with cover\r\n",
      "\r\n",
      "  --active                              enable active learning\r\n",
      "\r\n",
      "  --replay_b arg                        use experience replay at a specified \r\n",
      "                                        level [b=classification/regression, \r\n",
      "                                        m=multiclass, c=cost sensitive] with \r\n",
      "                                        specified buffer size\r\n",
      "\r\n",
      "  --baseline                            Learn an additive baseline (from \r\n",
      "                                        constant features) and a residual \r\n",
      "                                        separately in regression.\r\n",
      "\r\n",
      "  --OjaNewton                           Online Newton with Oja's Sketch\r\n",
      "\r\n",
      "  --bfgs                                use bfgs optimization\r\n",
      "\r\n",
      "  --conjugate_gradient                  use conjugate gradient based \r\n",
      "                                        optimization\r\n",
      "\r\n",
      "  --lda arg                             Run lda with <int> topics\r\n",
      "\r\n",
      "  --noop                                do no learning\r\n",
      "\r\n",
      "  --print                               print examples\r\n",
      "\r\n",
      "  --rank arg                            rank for matrix factorization.\r\n",
      "\r\n",
      "  --sendto arg                          send examples to <host>\r\n",
      "\r\n",
      "  --svrg                                Streaming Stochastic Variance Reduced \r\n",
      "                                        Gradient\r\n",
      "\r\n",
      "  --ftrl                                FTRL: Follow the Proximal Regularized \r\n",
      "                                        Leader\r\n",
      "\r\n",
      "  --pistol                              FTRL: Parameter-free Stochastic \r\n",
      "                                        Learning\r\n",
      "\r\n",
      "  --ksvm                                kernel svm\r\n",
      "\r\n",
      "Gradient Descent options:\r\n",
      "  --sgd                                 use regular stochastic gradient descent\r\n",
      "                                        update.\r\n",
      "  --adaptive                            use adaptive, individual learning \r\n",
      "                                        rates.\r\n",
      "  --adax                                use adaptive learning rates with x^2 \r\n",
      "                                        instead of g^2x^2\r\n",
      "  --invariant                           use safe/importance aware updates.\r\n",
      "  --normalized                          use per feature normalized updates\r\n",
      "  --sparse_l2 arg (=0)                  use per feature normalized updates\r\n",
      "\r\n",
      "Input options:\r\n",
      "  -d [ --data ] arg                     Example Set\r\n",
      "  --daemon                              persistent daemon mode on port 26542\r\n",
      "  --foreground                          in persistent daemon mode, do not run \r\n",
      "                                        in the background\r\n",
      "  --port arg                            port to listen on; use 0 to pick unused\r\n",
      "                                        port\r\n",
      "  --num_children arg                    number of children for persistent \r\n",
      "                                        daemon mode\r\n",
      "  --pid_file arg                        Write pid file in persistent daemon \r\n",
      "                                        mode\r\n",
      "  --port_file arg                       Write port used in persistent daemon \r\n",
      "                                        mode\r\n",
      "  -c [ --cache ]                        Use a cache.  The default is \r\n",
      "                                        <data>.cache\r\n",
      "  --cache_file arg                      The location(s) of cache_file.\r\n",
      "  --json                                Enable JSON parsing.\r\n",
      "  --dsjson                              Enable Decision Service JSON parsing.\r\n",
      "  -k [ --kill_cache ]                   do not reuse existing cache: create a \r\n",
      "                                        new one always\r\n",
      "  --compressed                          use gzip format whenever possible. If a\r\n",
      "                                        cache file is being created, this \r\n",
      "                                        option creates a compressed cache file.\r\n",
      "                                        A mixture of raw-text & compressed \r\n",
      "                                        inputs are supported with \r\n",
      "                                        autodetection.\r\n",
      "  --no_stdin                            do not default to reading from stdin\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!vw --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "62f9eb36-c321-43bb-823b-16e0dfa807fd",
    "_uuid": "e1af0fd07d8be18a75d911a2ca939b8af9839cf2"
   },
   "source": [
    "Vowpal Wabbit reads data from files or from standard input stream (stdin) assuming the following format:\n",
    "\n",
    "`[Label] [Importance] [Tag]|Namespace Features |Namespace Features ... |Namespace Features`\n",
    "\n",
    "`Namespace=String[:Value]`\n",
    "\n",
    "`Features=(String[:Value] )*`\n",
    "\n",
    "here [] denotes non-mandatory elements, and (...)\\* means some repeats. \n",
    "\n",
    "- **Label** is a number. In case of classification it is usually 1 and -1, in case of regression it is some real float value\n",
    "- **Importance** is a number, it denotes sample weight during training. Setting this helps when working with imbalanced data.\n",
    "- **Tag** is some string without spaces - it is a \"name\" of sample, VW saves it upon prediction. In order to separate Tag and Importance it is better to start Tag with the ' character.\n",
    "- **Namespace** is for creation of separate feature spaces. \n",
    "- **Features** are object features inside **Namespace**. Features have weight 1.0 by default, but it can be changed, for example - feature:0.1. \n",
    "\n",
    "\n",
    "The following string matches the VW format:\n",
    "\n",
    "```\n",
    "1 1.0 |Subject WHAT car is this |Organization University of Maryland:0.5 College Park\n",
    "```\n",
    "\n",
    "\n",
    "Let's check it and run VW with this training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "5cbedd43-8d8e-4577-9a67-4f2879f0666c",
    "_uuid": "250fa571012a53fbe15f9b16bd88244dae4f805a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = \r\n",
      "num sources = 1\r\n",
      "average  since         example        example  current  current  current\r\n",
      "loss     last          counter         weight    label  predict features\r\n",
      "1.000000 1.000000            1            1.0   1.0000   0.0000       10\r\n",
      "\r\n",
      "finished run\r\n",
      "number of examples per pass = 1\r\n",
      "passes used = 1\r\n",
      "weighted example sum = 1.000000\r\n",
      "weighted label sum = 1.000000\r\n",
      "average loss = 1.000000\r\n",
      "best constant = 1.000000\r\n",
      "best constant's loss = 0.000000\r\n",
      "total feature number = 10\r\n"
     ]
    }
   ],
   "source": [
    "! echo '1 1.0 |Subject WHAT car is this |Organization University of Maryland:0.5 College Park' | vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "95562a45-b8e2-45d0-be5c-5780f051ecb4",
    "_uuid": "e5f0a612859c499651f051fd78df3d34ad2e3c30"
   },
   "source": [
    "# 4. VW and Spooky Author Identification\n",
    "And finally, we'll try to use Vowpal Wabbit in the task of identifiyng one of the three authors (Edgar Allan Poe, Mary Shelley, or HP Lovecraft) given pieces of their spooky texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1b1964c-cf0e-4088-ade7-21006e0c874c",
    "_uuid": "0c2aab70a5a745e06cc8bcde84c9e6fb08a5cc69"
   },
   "source": [
    "Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "5f040180-4640-4206-8abd-339d8ce61c93",
    "_uuid": "0b44a05dad55dfc07145f6a6d5b3b6fb9a394ffe"
   },
   "outputs": [],
   "source": [
    "train_texts = pd.read_csv('../Data/train.csv', index_col='id')\n",
    "test_texts = pd.read_csv('../Data/test.csv', index_col='id')\n",
    "sample_sub = pd.read_csv('../Data/sample_submission.csv', \n",
    "                         index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cfc2f205-d120-403f-819d-12fa0e1a2b08",
    "_uuid": "3f2c78853a94a9b426d92654d00b20e0c8a980d2"
   },
   "source": [
    "Let's encode the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "0f6de304-1a52-42b1-b97c-760dcd2fcd44",
    "_uuid": "d388cb3353b13c229ef9223e5c0e2e914a2aca50"
   },
   "outputs": [],
   "source": [
    "author_code = {\"EAP\": 1, \"MWS\": 2,\"HPL\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "aaaf81ea-f174-4d9e-8a95-740de6d4ce08",
    "_uuid": "2b4e7c330ca91323e00d2debf5bd8c72fd100a3f"
   },
   "outputs": [],
   "source": [
    "train_texts[\"author_code\"] = train_texts[\"author\"].map(author_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7b351e54-0547-4a03-bf04-d7afdaa318f4",
    "_uuid": "98bb8fbf4852a0594847a111b65308c71e868ad5"
   },
   "source": [
    "This is going to be our simple validation scheme, we are just using the validation hold-out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "d19fa4ab-5a54-47a0-83b1-aadc9f8a9ea5",
    "_uuid": "994972e46bf3a6dd142015b7d62549432635fc5b"
   },
   "outputs": [],
   "source": [
    "train_texts_part, valid_texts = train_test_split(train_texts, test_size=0.01, random_state=17, \n",
    "                                                 stratify=train_texts[\"author_code\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "87679d34-43c2-4104-816f-2415ce4d4c17",
    "_uuid": "a1a9cee3e358d9d536dccbd40ecdcc8487233f1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19383, 196)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_part.shape[0], valid_texts.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "43a88ac0-dfca-4c8e-b40b-b7cea4e27b83",
    "_uuid": "afe94491005fb18e4b415bc6256a7c5a0dd08557",
    "collapsed": true
   },
   "source": [
    "To begin with, we are using only texts as features. Th following code will prepare the data to be fit into VW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "180cf600-c229-4735-a918-c59a88fe5754",
    "_uuid": "fff31a86630dc69a06893454302db46fba99105a"
   },
   "outputs": [],
   "source": [
    "def to_vw_only_text(out_vw, df, is_train=True):\n",
    "    with open(out_vw, \"w\") as out:\n",
    "        for i in range(df.shape[0]):\n",
    "            \n",
    "            if is_train:\n",
    "                target = df[\"author_code\"].iloc[i]\n",
    "            else:\n",
    "                # for the test set we can pick any target label â€“ we don't need it actually\n",
    "                target = 1 \n",
    "                       \n",
    "            # remove special VW symbols\n",
    "            text = df[\"text\"].iloc[i].strip().replace('|', '').replace(':', '').lower() \n",
    "            # leave only words of 3 and more chars\n",
    "            words = re.findall(\"\\w{3,}\", text) \n",
    "            new_text = \" \".join(words) \n",
    "\n",
    "            s = \"{} |text {}\\n\".format(target, new_text)\n",
    "\n",
    "            out.write(s)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "8ef5e13d-97f4-4379-b71d-f9e38991f23d",
    "_uuid": "c94295434fd5d437368a79272bb90bc2ab65b48a"
   },
   "outputs": [],
   "source": [
    "to_vw_only_text(\"../Data/vowpal_wabbit/train_part_only_text.vw\", train_texts_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "88f178af-3918-457d-ad09-b2cff316bd3f",
    "_uuid": "6f64f2a7725c871aab6c72c325df28f441110800",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 |text solitude only shall myself solitude shall thine\r\n",
      "2 |text knowledge the worldly principles lord raymond would have ever prevented from applying him however deep distress might have been\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 $PATH_TO_ALL_DATA/vowpal_wabbit/train_part_only_text.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "780fcf80-f771-4619-b68f-441d51f9c9a0",
    "_uuid": "4a7f170cf0cc0b0719d4c9b53cdc1a49f126a121"
   },
   "outputs": [],
   "source": [
    "to_vw_only_text(\"../Data/vowpal_wabbit/valid_only_text.vw\", valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "ff8d10e9-c6d9-4f8f-b316-759aa3ea0fa9",
    "_uuid": "4e0970e6ea1231fa365ab016e281ee9677fe7a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 |text were interrupted attendant who announced that the staff raymond was assembled the council chamber\r\n",
      "1 |text was not that feared look upon things horrible but that grew aghast lest there should nothing see\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 $PATH_TO_ALL_DATA/vowpal_wabbit/valid_only_text.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "9b4ca160-eb19-4df4-9b5f-81203491a847",
    "_uuid": "5faa7a7a017e14d76a7b6a42b1d6fa9350ad601c"
   },
   "outputs": [],
   "source": [
    "to_vw_only_text(\"../Data/vowpal_wabbit/train_only_text.vw\", train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "80d5621d-fff7-4a82-9ce8-60fd96b4b6d0",
    "_uuid": "0712ca776ea1ee9d72fadef546af2257c9eec69c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 |text this process however afforded means ascertaining the dimensions dungeon might make its circuit and return the point whence set out without being aware the fact perfectly uniform seemed the wall\r\n",
      "3 |text never once occurred that the fumbling might mere mistake\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 $PATH_TO_ALL_DATA/vowpal_wabbit/train_only_text.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "969b682c-9c82-48a0-9d29-a994d99ba4c4",
    "_uuid": "9b2dd32dfd675560a0994dd2ec88b3d6b4c697fa"
   },
   "outputs": [],
   "source": [
    "to_vw_only_text(\"../Data/vowpal_wabbit/test_only_text.vw\", test_texts, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "00fafdf1-61b3-499d-b0e4-6f0355a0b232",
    "_uuid": "0cb5f3918f9d44369dcd1c0c727d87cae5d8b7a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 |text still urged our leaving ireland with such inquietude and impatience father thought best yield\r\n",
      "1 |text fire wanted fanning could readily fanned with newspaper and the government grew weaker have doubt that leather and iron acquired durability proportion for very short time there was not pair bellows all rotterdam that ever stood need stitch required the assistance hammer\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 $PATH_TO_ALL_DATA/vowpal_wabbit/test_only_text.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e64e7c66-82d8-4fdd-8dda-f670e112ec40",
    "_uuid": "0aa31bb86f8f80931b4403fcc68ba747e708e2bf"
   },
   "source": [
    "Here we train a VW model (actuall 3 one-against-all models), we use 28 bits for feature hashing resulting in $2^{28} \\approx 2.7 \\times 10^8$ features. The loss is set to logistic as it's works well for classification (and it's also our evaluation metric in the competition). We incorporate bigrams and perform 10 passes over the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "6d76a679-9d5e-4409-ba0f-eebbf0da8af0",
    "_uuid": "105f8896584e606b85b0ccfa43dbd6cd4ebf7f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = ../Data/vowpal_wabbit/model_only_text_part.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../Data/vowpal_wabbit/train_part_only_text.vw.cache\n",
      "Reading datafile = ../Data/vowpal_wabbit/train_part_only_text.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        1       14\n",
      "0.500000 0.000000            2            2.0        2        2       38\n",
      "0.500000 0.500000            4            4.0        3        2       54\n",
      "0.375000 0.250000            8            8.0        3        3       80\n",
      "0.500000 0.625000           16           16.0        1        3       56\n",
      "0.531250 0.562500           32           32.0        2        3       74\n",
      "0.593750 0.656250           64           64.0        1        2       28\n",
      "0.515625 0.437500          128          128.0        1        2       12\n",
      "0.515625 0.515625          256          256.0        3        2       50\n",
      "0.482422 0.449219          512          512.0        2        2       72\n",
      "0.436523 0.390625         1024         1024.0        1        3       36\n",
      "0.387695 0.338867         2048         2048.0        1        1       56\n",
      "0.328857 0.270020         4096         4096.0        2        1        6\n",
      "0.277222 0.225586         8192         8192.0        1        1       70\n",
      "0.229675 0.182129        16384        16384.0        1        1       24\n",
      "0.207418 0.207418        32768        32768.0        2        2       16 h\n",
      "0.183516 0.159615        65536        65536.0        1        1       26 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 17445\n",
      "passes used = 7\n",
      "weighted example sum = 122115.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.155831 h\n",
      "total feature number = 5110364\n"
     ]
    }
   ],
   "source": [
    "!vw --oaa 3 ../Data/vowpal_wabbit/train_part_only_text.vw -f ../Data/vowpal_wabbit/model_only_text_part.vw -b 28 --random_seed 17 \\\n",
    "--loss_function logistic --ngram 2 --passes 10 -k -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "0ff6027a-01c7-40bc-8ced-da4a6fbdd898",
    "_uuid": "a566e1f5c55210ee234d15e875e7d4e44d010f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../Data/vowpal_wabbit/valid_pred1.txt\n",
      "raw predictions = ../Data/vowpal_wabbit/valid_prob1.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../Data/vowpal_wabbit/valid_only_text.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        2        2       28\n",
      "0.000000 0.000000            2            2.0        1        1       34\n",
      "0.000000 0.000000            4            4.0        3        3       46\n",
      "0.000000 0.000000            8            8.0        1        1       32\n",
      "0.062500 0.125000           16           16.0        2        2       56\n",
      "0.062500 0.062500           32           32.0        2        2       10\n",
      "0.125000 0.187500           64           64.0        2        2       48\n",
      "0.132812 0.140625          128          128.0        3        3       72\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 196\n",
      "passes used = 1\n",
      "weighted example sum = 196.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.142857\n",
      "total feature number = 7368\n",
      "CPU times: user 6.71 ms, sys: 8.43 ms, total: 15.1 ms\n",
      "Wall time: 362 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i ../Data/vowpal_wabbit/model_only_text_part.vw -t -d ../Data/vowpal_wabbit/valid_only_text.vw -p ../Data/vowpal_wabbit/valid_pred1.txt --random_seed 17 -r ../Data/vowpal_wabbit/valid_prob1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e8ce976-cf6e-496a-8032-e7181b09cedd",
    "_uuid": "d0fdbc4d36dbd085647593df674fb66bcdedd5fd"
   },
   "source": [
    "We get classification scores for each validation sample, so we'll perform sigmoid transformation to map them into [0,1] range. Further, we calculate the logistic loss between target vector in validation data set and the transformed predictions. It's handy to write a special function for all these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "61c00a14-e0a8-4bd2-8aba-b7472e306bda",
    "_uuid": "a196e84afc70c238382539e72c208296ddd7eeaa"
   },
   "outputs": [],
   "source": [
    "def evaluate_vw_prediction(path_to_vw_pred_probs, is_test=False, target=None, write_submission=False,\n",
    "                          submission_file=None, test_index=test_texts.index, columns=['EAP', 'MWS', 'HPL']):\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z)) \n",
    "    \n",
    "    with open(path_to_vw_pred_probs) as pred_file:\n",
    "        pred_probs =  np.array([[float(pair.split(':')[1]) for pair in line.strip().split()] \n",
    "                         for line in pred_file.readlines()])\n",
    "        pred_probs  = sigmoid(pred_probs)\n",
    "        \n",
    "        if target is not None and not is_test:\n",
    "            print(log_loss(target, pred_probs))\n",
    "        \n",
    "        if write_submission and submission_file is not None:\n",
    "            subm_df = pd.DataFrame(pred_probs, columns=columns)\n",
    "            subm_df.index = test_index\n",
    "            subm_df.to_csv(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "e33a4b6b-a41a-4edd-8365-bfd9a5e7743f",
    "_uuid": "c1312736898b93471cd9adb0f11e7742ae33d287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.473879356865\n"
     ]
    }
   ],
   "source": [
    "evaluate_vw_prediction(os.path.join(PATH_TO_ALL_DATA, 'vowpal_wabbit/valid_prob1.txt'), \n",
    "                       target=valid_texts['author_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6752069-d5f1-422c-a6f4-a250050a1caa",
    "_uuid": "0a3a26446bcadea96151541f14fc1631fe87d241"
   },
   "source": [
    "Now it's high time to train VW on the full training set, make predictions for the test set and submit them to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "7c7b539e-5759-4b98-8015-dda061315b51",
    "_uuid": "91ecaa2ae92ee51cb88823c3492ec8a37cf5396f"
   },
   "outputs": [],
   "source": [
    "!vw --oaa 3 ../Data/vowpal_wabbit/train_only_text.vw -f ../Data/vowpal_wabbit/model_only_text.vw -b 28 --random_seed 17 \\\n",
    "--loss_function logistic --ngram 2 --passes 10 -k -c --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "73f604ce-65f1-43e3-820e-e821384ac3c7",
    "_uuid": "3ff50fde59e4cdcfef60807d3d526417573140f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.68 ms, sys: 9.57 ms, total: 19.2 ms\n",
      "Wall time: 666 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i ../Data/vowpal_wabbit/model_only_text.vw -t -d ../Data/vowpal_wabbit/test_only_text.vw -p ../Data/vowpal_wabbit/test_pred1.txt --random_seed 17 -r ../Data/vowpal_wabbit/test_prob1.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "8e57f895-ae5d-41ea-b939-7a5cba978a23",
    "_uuid": "c5b4a182dcad5803508c40b562d6db766745e46f"
   },
   "outputs": [],
   "source": [
    "evaluate_vw_prediction(os.path.join(PATH_TO_ALL_DATA, 'vowpal_wabbit/test_prob1.txt'), \n",
    "                       is_test=True, write_submission=True,\n",
    "                       submission_file='../predictions/submission1_only_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "2c40de5d-cb99-4f07-939f-b9b0a8c2a6e1",
    "_uuid": "9ab0ff333260a630abe8f6f6e2d695197b2cdc57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,EAP,MWS,HPL\r\n",
      "id02310,0.21302281257134767,0.7462194862375665,0.02739253612496469\r\n",
      "id24541,0.8236463649690904,0.0062378023959290705,0.143383189058737\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 ../predictions/submission1_only_text.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e2ded01a-a2a4-4c49-8916-444ce60f40c0",
    "_uuid": "8aa060b5652870834de9bea7bec7683f8a8349a5"
   },
   "source": [
    "With this submission we get 0.43187 on the [Public Leaderboard](https://www.kaggle.com/c/spooky-author-identification/leaderboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "649f265c-5171-49fe-8b7d-599f1ac7565c",
    "_uuid": "a3cd115f1102ab0a039715400684a7dd04d2325f"
   },
   "source": [
    "Let's add some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "968a402a-f7ba-41a8-a37b-ad99e8ff1470",
    "_uuid": "d9d18e4023e0a5e2172e677cf3d2f6689dcd72cf"
   },
   "outputs": [],
   "source": [
    "max_words_in_text = train_texts['text'].apply(lambda text: len(re.findall(\"\\w{3,}\", text.strip()))).max()\n",
    "max_unique_words_in_text = train_texts['text'].apply(lambda text: len(set(re.findall(\"\\w{3,}\", text.strip())))).max()\n",
    "max_aver_word_len_in_text = train_texts['text'].apply(lambda text: \n",
    "                                                      sum([len(w) for w in re.findall(\"\\w{3,}\", text.strip())]) / \n",
    "                                                      len(re.findall(\"\\w{3,}\", text.strip()))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "14817e88-eca6-4d34-bc2c-0c2548f040a3",
    "_uuid": "d0bcdd037f3de3afbb95bb8f8f9630f8b9355cdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(667, 400, 11.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words_in_text, max_unique_words_in_text, max_aver_word_len_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "ba9fda37-1d11-4bf5-b47f-a5b59fce309a",
    "_uuid": "3cf86e42dc125e5dce443a8b967b051c6bc8e28b"
   },
   "outputs": [],
   "source": [
    "def to_vw_text_and_some_features(out_vw, df, is_train=True):\n",
    "    with open(out_vw, \"w\") as out:\n",
    "        for i in range(df.shape[0]):\n",
    "            \n",
    "            if is_train:\n",
    "                target = df[\"author_code\"].iloc[i]\n",
    "            else:\n",
    "                # for the test set we can pick any target label â€“ we don't need it actually\n",
    "                target = 1 \n",
    "                       \n",
    "            # remove special VW symbols\n",
    "            text = df[\"text\"].iloc[i].strip().replace('|', '').replace(':', '').lower() \n",
    "            # leave only words of 3 and more chars\n",
    "            words = re.findall(\"\\w{3,}\", text) \n",
    "            new_text = \" \".join(words)    \n",
    "            \n",
    "            num_words = round(len(words) / max_words_in_text, 4)\n",
    "            num_uniq_words = round(len(set(words)) / max_unique_words_in_text, 4)\n",
    "            aver_word_len = round(sum([len(w) for w in words]) / len(words) / max_aver_word_len_in_text, 4)\n",
    "\n",
    "            features = [num_words, num_uniq_words, aver_word_len] \n",
    "            features_vw = ' '.join(['{}:{}'.format(i[0], i[1]) for i in zip(range(len(features)), features)])\n",
    "            s = \"{} |text {} |num {}\\n\".format(target, new_text, features_vw)\n",
    "\n",
    "            out.write(s)   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "a3f1c58b-f4b5-4143-a5c1-a16c42fe45e6",
    "_uuid": "c67071e9c216fc5ad67a56c2fd1ad7d56500927e"
   },
   "outputs": [],
   "source": [
    "to_vw_text_and_some_features(\"../Data/vowpal_wabbit/train_part_text_feat.vw\", train_texts_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "e495d126-24ac-4b9e-a186-6e09165f3efa",
    "_uuid": "2476e90b19002504292954007a8e611d9e725b89",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: ../Data//train_part_text_feat.vw: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 $PATH_TO_ALL_DATA/train_part_text_feat.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "ab80886c-482c-4bb0-bf69-e321ad31423b",
    "_uuid": "538fbd0487d851a5ef16dc31cbb8db119a003b76"
   },
   "outputs": [],
   "source": [
    "to_vw_text_and_some_features(\"../Data/vowpal_wabbit/valid_text_feat.vw\", valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "5c27ce62-cd00-44f5-acd0-0a0ee22075ec",
    "_uuid": "7229cde8e7dcd6e62f751d59c37d5f6600c6afd9"
   },
   "outputs": [],
   "source": [
    "to_vw_text_and_some_features(\"../Data/vowpal_wabbit/train_text_feat.vw\", train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "a1b091dd-f1ed-4fa4-88fc-3df6f37036f5",
    "_uuid": "82d864847f6eeb10961979126a6cf4480ffb556f"
   },
   "outputs": [],
   "source": [
    "to_vw_text_and_some_features(\"../Data/vowpal_wabbit/test_text_feat.vw\", test_texts, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "4c6cd4f9-3671-4b50-81b0-e19263122c78",
    "_uuid": "e6a23612d869c028552e9b70e86b5a6d8394d936"
   },
   "outputs": [],
   "source": [
    "!vw --oaa 3 ../Data/vowpal_wabbit/train_part_text_feat.vw -f ../Data/vowpal_wabbit/model_text_feat_part.vw -b 28 --random_seed 17 \\\n",
    "--loss_function logistic --ngram 2 --passes 10 -k -c --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "34a298a2-307b-4741-84ef-47cd22cb50e6",
    "_uuid": "4b85a6f41289744decd18f2d2eba4da10ece0301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.24 ms, sys: 9.11 ms, total: 14.3 ms\n",
      "Wall time: 376 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i ../Data/vowpal_wabbit/model_text_feat_part.vw -t -d ../Data/vowpal_wabbit/valid_text_feat.vw -p ../Data/vowpal_wabbit/valid_pred2.txt --random_seed 17 -r ../Data/vowpal_wabbit/valid_prob2.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "9b006652-15c4-4371-9a12-01d84c369de7",
    "_uuid": "aea1007d4a5adc6d15487b345a25095e27ab3128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.438015789069\n"
     ]
    }
   ],
   "source": [
    "evaluate_vw_prediction(os.path.join(PATH_TO_ALL_DATA, 'vowpal_wabbit/valid_prob2.txt'), \n",
    "                       target=valid_texts['author_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "8c29bc10-3045-4ea9-8657-a3bc60881330",
    "_uuid": "ae90c28dc310cbfd34d18ca287c1f131b5caba6b"
   },
   "outputs": [],
   "source": [
    "!vw --oaa 3 ../Data/vowpal_wabbit/train_text_feat.vw -f ../Data/vowpal_wabbit/model_text_feat.vw -b 28 --random_seed 17 \\\n",
    "--loss_function logistic --ngram 2 --passes 10 -k -c --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "b64a49e8-ef85-4ecc-8f2d-f4c48afc7969",
    "_uuid": "83da202271bbd4e33380c1cabf0b2a79d899e609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 ms, sys: 9.15 ms, total: 20.1 ms\n",
      "Wall time: 657 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i ../Data/vowpal_wabbit/model_text_feat.vw -t -d ../Data/vowpal_wabbit/test_text_feat.vw -p ../Data/vowpal_wabbit/test_pred2.txt --random_seed 17 -r ../Data/vowpal_wabbit/test_prob2.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "a82a1464-d5f2-481f-acbf-ed445fc697b8",
    "_uuid": "f0b4eae6df8f8a7eb6ce902aa02a228ea8b442bd"
   },
   "outputs": [],
   "source": [
    "evaluate_vw_prediction(os.path.join(PATH_TO_ALL_DATA, 'vowpal_wabbit/test_prob2.txt'), \n",
    "                       is_test=True, write_submission=True,\n",
    "                       submission_file='../predictions/submission2_text_feat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ee985d4-bf27-44ee-b201-366e31f7aeb4",
    "_uuid": "b3ae8ee01c9ae031e6da313dfc30ad7e9dea637e"
   },
   "source": [
    "With this we get 0.43267 on public LB so it doesn't seem like a major improvement. However,  we'll take into account that our holdout score estimate is calculated with 5874 samples while the public leaderboard one â€“ with $\\approx$ 2517 samples (30% $\\times$ 8392). Finally, we'll calculate the weighted sum of CV and LB scores to evaluate our submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "11d0f499-ccbc-4685-982c-9c17f302dbef",
    "_uuid": "f39c17df64e3a2bbf90e7e252dc7afd8e6db585d"
   },
   "outputs": [],
   "source": [
    "def validate_submission_local_and_lb_mix(local_score, public_lb_score, local_size=5874, public_lb_size=2517):\n",
    "    return 1. / (local_size + public_lb_size) * (local_size * local_score +\n",
    "                                                public_lb_size * public_lb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "c30b1cb4-8cdd-422b-aa4d-e0efc491c7bd",
    "_uuid": "4c8fc7c792503840ac1d772f1fa8dbd7b24a46d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4652197032534859"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first submission\n",
    "validate_submission_local_and_lb_mix(local_score=.47951, public_lb_score=.43187)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "33356dc8-6d1f-4ce8-b422-2f738a063bdb",
    "_uuid": "c93da2c2150f1e9d56a311197fc18b47e979bafc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45810229889166965"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second submission\n",
    "validate_submission_local_and_lb_mix(local_score=.469, \n",
    "                                      public_lb_score=.43267)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "91cbf31e-43a1-482d-af7a-02de3826b00f",
    "_uuid": "bf8903c882ff92061cb8ad971f097734254ed866"
   },
   "source": [
    "It seems 3 features helped here to lower logloss. However, feature engineering is not the main goal of this tutorial, there are already dozens of nice ones in [Kernels](https://www.kaggle.com/c/spooky-author-identification/kernels). \n",
    "\n",
    "You can experiment with lots of other features, bunches of features and techniques (word2vec, LDA, topic modelling, to name just a few possible approaches). Luckily, Vowpal Wabbit serves ideally for performing lots of \"design-implement-check\" iterations. Try it out and you'll definitely gain a new helpful skill!\n",
    "\n",
    "## Useful links\n",
    "- Official VW [documentation](https://github.com/JohnLangford/vowpal_wabbit/wiki) on Github\n",
    "- [Chapter](http://www.deeplearningbook.org/contents/numerical.html) \"Numeric Computation\" of the [Deep Learning book](http://www.deeplearningbook.org/)\n",
    "- \"Command-line Tools can be 235x Faster than your Hadoop Cluster\", [post](https://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html)\n",
    "- Benchmarking various ML algorithms on Criteo 1TB dataset, [GitHub](https://github.com/rambler-digital-solutions/criteo-1tb-benchmark)\n",
    "- [FastML.com](http://fastml.com/blog/categories/vw/), category VW\n",
    "\n",
    "<img src=\"https://habrastorage.org/webt/_r/lz/wb/_rlzwbzedhlivdnhvfzk1apnzss.jpeg\" width=50%>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
